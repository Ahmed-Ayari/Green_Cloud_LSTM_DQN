{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3fd4b878",
   "metadata": {},
   "source": [
    "## üîß Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeb2e851",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append('../src')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "\n",
    "from ml_models.dqn_agent import DQNAgent, Experience\n",
    "from ml_models.hybrid_controller import HybridController\n",
    "from ml_models.data_preprocessing import WorkloadDataLoader\n",
    "\n",
    "# Configure plotting\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"‚úÖ Imports completed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb1fef3c",
   "metadata": {},
   "source": [
    "## üèóÔ∏è Initialize Environment and Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "371b6f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment configuration\n",
    "NUM_HOSTS = 10\n",
    "NUM_VMS = 50\n",
    "SEQUENCE_LENGTH = 10\n",
    "\n",
    "# DQN configuration\n",
    "state_size = NUM_HOSTS * 2 + 1  # Current + predicted utilization + avg\n",
    "action_size = NUM_HOSTS * NUM_HOSTS + 1  # All possible migrations + do_nothing\n",
    "\n",
    "# Initialize DQN agent\n",
    "dqn_agent = DQNAgent(\n",
    "    state_size=state_size,\n",
    "    action_size=action_size,\n",
    "    learning_rate=0.001,\n",
    "    gamma=0.99,\n",
    "    epsilon=1.0,\n",
    "    epsilon_decay=0.995,\n",
    "    epsilon_min=0.01\n",
    ")\n",
    "\n",
    "# Initialize hybrid controller\n",
    "controller = HybridController(\n",
    "    num_hosts=NUM_HOSTS,\n",
    "    num_vms=NUM_VMS,\n",
    "    sequence_length=SEQUENCE_LENGTH\n",
    ")\n",
    "\n",
    "print(f\"\\nüìä Configuration:\")\n",
    "print(f\"State size: {state_size}\")\n",
    "print(f\"Action size: {action_size}\")\n",
    "print(f\"Hosts: {NUM_HOSTS}\")\n",
    "print(f\"VMs: {NUM_VMS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67ab3175",
   "metadata": {},
   "source": [
    "## üì• Prepare Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aab72088",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate workload data\n",
    "data_loader = WorkloadDataLoader()\n",
    "workload_data = data_loader.generate_synthetic_workload(\n",
    "    num_hosts=NUM_HOSTS,\n",
    "    num_timesteps=1000,\n",
    "    pattern='mixed'\n",
    ")\n",
    "\n",
    "# Split into training and test\n",
    "train_size = int(0.8 * len(workload_data))\n",
    "train_workload = workload_data[:train_size]\n",
    "test_workload = workload_data[train_size:]\n",
    "\n",
    "print(f\"Training timesteps: {len(train_workload)}\")\n",
    "print(f\"Test timesteps: {len(test_workload)}\")\n",
    "\n",
    "# Visualize training data sample\n",
    "plt.figure(figsize=(15, 6))\n",
    "for i in range(min(5, NUM_HOSTS)):\n",
    "    plt.plot(train_workload[:200, i], label=f'Host {i}', linewidth=1.5)\n",
    "plt.title('Training Workload Sample (First 200 Timesteps)', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Timestep')\n",
    "plt.ylabel('CPU Utilization (%)')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a29d52e",
   "metadata": {},
   "source": [
    "## üèãÔ∏è Train DQN Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03cdbd4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training configuration\n",
    "NUM_EPISODES = 100\n",
    "STEPS_PER_EPISODE = 100\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# Tracking metrics\n",
    "episode_rewards = []\n",
    "episode_energies = []\n",
    "episode_sla_violations = []\n",
    "episode_migrations = []\n",
    "episode_losses = []\n",
    "\n",
    "print(\"\\nüöÄ Starting DQN Training...\\n\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for episode in tqdm(range(NUM_EPISODES), desc=\"Training Episodes\"):\n",
    "    # Reset episode metrics\n",
    "    episode_reward = 0\n",
    "    episode_energy = 0\n",
    "    episode_sla = 0\n",
    "    episode_mig = 0\n",
    "    \n",
    "    # Random starting point in training data\n",
    "    start_idx = np.random.randint(0, len(train_workload) - STEPS_PER_EPISODE)\n",
    "    \n",
    "    for step in range(STEPS_PER_EPISODE):\n",
    "        # Get current utilization\n",
    "        current_utilization = train_workload[start_idx + step].tolist()\n",
    "        \n",
    "        # Get state from controller\n",
    "        state = controller.get_current_state(current_utilization)\n",
    "        \n",
    "        # Select action\n",
    "        action = dqn_agent.select_action(state)\n",
    "        \n",
    "        # Execute step\n",
    "        action_result, reward, metrics = controller.step(current_utilization)\n",
    "        \n",
    "        # Get next state\n",
    "        next_utilization = train_workload[start_idx + step + 1].tolist() if step < STEPS_PER_EPISODE - 1 else current_utilization\n",
    "        next_state = controller.get_current_state(next_utilization)\n",
    "        \n",
    "        # Store experience\n",
    "        done = (step == STEPS_PER_EPISODE - 1)\n",
    "        dqn_agent.store_experience(state, action, reward, next_state, done)\n",
    "        \n",
    "        # Train agent\n",
    "        if len(dqn_agent.replay_buffer) >= BATCH_SIZE:\n",
    "            loss = dqn_agent.train(BATCH_SIZE)\n",
    "            if loss is not None:\n",
    "                episode_losses.append(loss)\n",
    "        \n",
    "        # Track metrics\n",
    "        episode_reward += reward\n",
    "        episode_energy += metrics['energy']\n",
    "        episode_sla += metrics['sla_violations']\n",
    "        episode_mig += metrics['migrations']\n",
    "    \n",
    "    # Store episode metrics\n",
    "    episode_rewards.append(episode_reward)\n",
    "    episode_energies.append(episode_energy)\n",
    "    episode_sla_violations.append(episode_sla)\n",
    "    episode_migrations.append(episode_mig)\n",
    "    \n",
    "    # Update target network\n",
    "    if episode % 10 == 0:\n",
    "        dqn_agent.update_target_network()\n",
    "    \n",
    "    # Print progress\n",
    "    if (episode + 1) % 10 == 0:\n",
    "        avg_reward = np.mean(episode_rewards[-10:])\n",
    "        avg_energy = np.mean(episode_energies[-10:])\n",
    "        print(f\"\\nEpisode {episode + 1}/{NUM_EPISODES}\")\n",
    "        print(f\"  Avg Reward (last 10): {avg_reward:.2f}\")\n",
    "        print(f\"  Avg Energy (last 10): {avg_energy:.2f}W\")\n",
    "        print(f\"  Epsilon: {dqn_agent.epsilon:.4f}\")\n",
    "        print(\"=\" * 70)\n",
    "\n",
    "print(\"\\n‚úÖ Training completed!\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2ce2500",
   "metadata": {},
   "source": [
    "## üìä Training Progress Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d2248a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# 1. Episode Rewards\n",
    "axes[0, 0].plot(episode_rewards, alpha=0.3, color='blue')\n",
    "axes[0, 0].plot(pd.Series(episode_rewards).rolling(window=10).mean(), linewidth=2, color='blue', label='Moving Avg (10)')\n",
    "axes[0, 0].set_title('Episode Rewards', fontsize=14, fontweight='bold')\n",
    "axes[0, 0].set_xlabel('Episode')\n",
    "axes[0, 0].set_ylabel('Total Reward')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Energy Consumption\n",
    "axes[0, 1].plot(episode_energies, alpha=0.3, color='red')\n",
    "axes[0, 1].plot(pd.Series(episode_energies).rolling(window=10).mean(), linewidth=2, color='red', label='Moving Avg (10)')\n",
    "axes[0, 1].set_title('Energy Consumption per Episode', fontsize=14, fontweight='bold')\n",
    "axes[0, 1].set_xlabel('Episode')\n",
    "axes[0, 1].set_ylabel('Energy (W)')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# 3. SLA Violations\n",
    "axes[1, 0].plot(episode_sla_violations, alpha=0.3, color='orange')\n",
    "axes[1, 0].plot(pd.Series(episode_sla_violations).rolling(window=10).mean(), linewidth=2, color='orange', label='Moving Avg (10)')\n",
    "axes[1, 0].set_title('SLA Violations per Episode', fontsize=14, fontweight='bold')\n",
    "axes[1, 0].set_xlabel('Episode')\n",
    "axes[1, 0].set_ylabel('Count')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# 4. VM Migrations\n",
    "axes[1, 1].plot(episode_migrations, alpha=0.3, color='green')\n",
    "axes[1, 1].plot(pd.Series(episode_migrations).rolling(window=10).mean(), linewidth=2, color='green', label='Moving Avg (10)')\n",
    "axes[1, 1].set_title('VM Migrations per Episode', fontsize=14, fontweight='bold')\n",
    "axes[1, 1].set_xlabel('Episode')\n",
    "axes[1, 1].set_ylabel('Count')\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../results/graphs/dqn_training_progress.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5afb50ba",
   "metadata": {},
   "source": [
    "## üìâ Loss Curve Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bc16425",
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(episode_losses) > 0:\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    plt.plot(episode_losses, alpha=0.3, color='purple')\n",
    "    plt.plot(pd.Series(episode_losses).rolling(window=100).mean(), linewidth=2, color='purple', label='Moving Avg (100)')\n",
    "    plt.title('DQN Training Loss', fontsize=14, fontweight='bold')\n",
    "    plt.xlabel('Training Step')\n",
    "    plt.ylabel('Loss (MSE)')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.savefig('../results/graphs/dqn_loss_curve.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"Final Average Loss (last 100 steps): {np.mean(episode_losses[-100:]):.4f}\")\n",
    "else:\n",
    "    print(\"No loss data available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a569fdb8",
   "metadata": {},
   "source": [
    "## üéØ Evaluate Trained Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc87dd25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation on test set\n",
    "print(\"\\nüß™ Evaluating trained DQN agent on test data...\\n\")\n",
    "\n",
    "eval_rewards = []\n",
    "eval_energies = []\n",
    "eval_sla = []\n",
    "eval_migrations = []\n",
    "\n",
    "# Set epsilon to 0 for evaluation (no exploration)\n",
    "original_epsilon = dqn_agent.epsilon\n",
    "dqn_agent.epsilon = 0.0\n",
    "\n",
    "for step in tqdm(range(len(test_workload) - 1), desc=\"Evaluating\"):\n",
    "    current_utilization = test_workload[step].tolist()\n",
    "    state = controller.get_current_state(current_utilization)\n",
    "    action = dqn_agent.select_action(state)\n",
    "    action_result, reward, metrics = controller.step(current_utilization)\n",
    "    \n",
    "    eval_rewards.append(reward)\n",
    "    eval_energies.append(metrics['energy'])\n",
    "    eval_sla.append(metrics['sla_violations'])\n",
    "    eval_migrations.append(metrics['migrations'])\n",
    "\n",
    "# Restore epsilon\n",
    "dqn_agent.epsilon = original_epsilon\n",
    "\n",
    "print(\"\\nüìä Evaluation Results:\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Total Reward: {sum(eval_rewards):.2f}\")\n",
    "print(f\"Avg Reward per step: {np.mean(eval_rewards):.2f}\")\n",
    "print(f\"Total Energy: {sum(eval_energies):.2f}W\")\n",
    "print(f\"Avg Energy per step: {np.mean(eval_energies):.2f}W\")\n",
    "print(f\"Total SLA Violations: {sum(eval_sla)}\")\n",
    "print(f\"Total Migrations: {sum(eval_migrations)}\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90082002",
   "metadata": {},
   "source": [
    "## üìà Evaluation Metrics Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bddfa45",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# 1. Rewards over time\n",
    "axes[0, 0].plot(eval_rewards, linewidth=1, alpha=0.7)\n",
    "axes[0, 0].axhline(np.mean(eval_rewards), color='red', linestyle='--', label=f'Mean: {np.mean(eval_rewards):.2f}')\n",
    "axes[0, 0].set_title('Reward per Timestep (Test Set)', fontsize=14, fontweight='bold')\n",
    "axes[0, 0].set_xlabel('Timestep')\n",
    "axes[0, 0].set_ylabel('Reward')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Energy consumption\n",
    "axes[0, 1].plot(eval_energies, linewidth=1, alpha=0.7, color='red')\n",
    "axes[0, 1].axhline(np.mean(eval_energies), color='blue', linestyle='--', label=f'Mean: {np.mean(eval_energies):.2f}W')\n",
    "axes[0, 1].set_title('Energy Consumption (Test Set)', fontsize=14, fontweight='bold')\n",
    "axes[0, 1].set_xlabel('Timestep')\n",
    "axes[0, 1].set_ylabel('Energy (W)')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Cumulative SLA violations\n",
    "cumulative_sla = np.cumsum(eval_sla)\n",
    "axes[1, 0].plot(cumulative_sla, linewidth=2, color='orange')\n",
    "axes[1, 0].set_title('Cumulative SLA Violations (Test Set)', fontsize=14, fontweight='bold')\n",
    "axes[1, 0].set_xlabel('Timestep')\n",
    "axes[1, 0].set_ylabel('Cumulative Count')\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Cumulative migrations\n",
    "cumulative_mig = np.cumsum(eval_migrations)\n",
    "axes[1, 1].plot(cumulative_mig, linewidth=2, color='green')\n",
    "axes[1, 1].set_title('Cumulative VM Migrations (Test Set)', fontsize=14, fontweight='bold')\n",
    "axes[1, 1].set_xlabel('Timestep')\n",
    "axes[1, 1].set_ylabel('Cumulative Count')\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../results/graphs/dqn_evaluation_metrics.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5b91752",
   "metadata": {},
   "source": [
    "## üîç Action Distribution Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12e97227",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze action distribution during evaluation\n",
    "action_counts = []\n",
    "dqn_agent.epsilon = 0.0\n",
    "\n",
    "for step in range(min(200, len(test_workload) - 1)):\n",
    "    current_utilization = test_workload[step].tolist()\n",
    "    state = controller.get_current_state(current_utilization)\n",
    "    action = dqn_agent.select_action(state)\n",
    "    action_counts.append(action)\n",
    "\n",
    "# Classify actions\n",
    "do_nothing_count = sum(1 for a in action_counts if a == action_size - 1)\n",
    "migration_count = len(action_counts) - do_nothing_count\n",
    "\n",
    "print(f\"\\nüéØ Action Distribution (200 timesteps):\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Do Nothing: {do_nothing_count} ({do_nothing_count/len(action_counts)*100:.2f}%)\")\n",
    "print(f\"Migrations: {migration_count} ({migration_count/len(action_counts)*100:.2f}%)\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Visualize action distribution\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Pie chart\n",
    "labels = ['Do Nothing', 'Migration']\n",
    "sizes = [do_nothing_count, migration_count]\n",
    "colors = ['#66b3ff', '#ff9999']\n",
    "axes[0].pie(sizes, labels=labels, autopct='%1.1f%%', colors=colors, startangle=90)\n",
    "axes[0].set_title('Action Type Distribution', fontsize=14, fontweight='bold')\n",
    "\n",
    "# Action histogram\n",
    "axes[1].hist(action_counts, bins=50, color='teal', edgecolor='black', alpha=0.7)\n",
    "axes[1].set_title('Action ID Distribution', fontsize=14, fontweight='bold')\n",
    "axes[1].set_xlabel('Action ID')\n",
    "axes[1].set_ylabel('Frequency')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../results/graphs/dqn_action_distribution.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa5c3447",
   "metadata": {},
   "source": [
    "## üíæ Save Trained Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b1ce4f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save DQN model\n",
    "os.makedirs('../models', exist_ok=True)\n",
    "dqn_agent.save('../models/dqn_agent.pth')\n",
    "\n",
    "# Save training metrics\n",
    "training_results = {\n",
    "    'episode_rewards': episode_rewards,\n",
    "    'episode_energies': episode_energies,\n",
    "    'episode_sla_violations': episode_sla_violations,\n",
    "    'episode_migrations': episode_migrations,\n",
    "    'episode_losses': episode_losses,\n",
    "    'eval_rewards': eval_rewards,\n",
    "    'eval_energies': eval_energies,\n",
    "    'eval_sla': eval_sla,\n",
    "    'eval_migrations': eval_migrations\n",
    "}\n",
    "\n",
    "import json\n",
    "with open('../results/metrics/dqn_training_results.json', 'w') as f:\n",
    "    # Convert numpy arrays to lists for JSON serialization\n",
    "    json_results = {k: [float(x) for x in v] if isinstance(v, list) else v for k, v in training_results.items()}\n",
    "    json.dump(json_results, f, indent=2)\n",
    "\n",
    "print(\"\\n‚úÖ Models and results saved successfully!\")\n",
    "print(\"   - dqn_agent.pth\")\n",
    "print(\"   - dqn_training_results.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92f2d176",
   "metadata": {},
   "source": [
    "## üìã Summary\n",
    "\n",
    "**Key Results:**\n",
    "- ‚úÖ DQN agent trained successfully over 100 episodes\n",
    "- ‚úÖ Multi-objective optimization: Balancing energy, SLA, and migrations\n",
    "- ‚úÖ Experience replay stabilizes learning\n",
    "- ‚úÖ Epsilon-greedy exploration converges to exploitation\n",
    "- ‚úÖ Evaluation shows consistent performance on test data\n",
    "\n",
    "**Performance Metrics:**\n",
    "- Total Reward: {sum(eval_rewards):.2f}\n",
    "- Total Energy: {sum(eval_energies):.2f}W\n",
    "- Total SLA Violations: {sum(eval_sla)}\n",
    "- Total Migrations: {sum(eval_migrations)}\n",
    "\n",
    "**Next Steps:**\n",
    "1. Integrate with LSTM predictions (hybrid system)\n",
    "2. Compare with baseline algorithms\n",
    "3. Generate final results visualization (see `04_results_visualization.ipynb`)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

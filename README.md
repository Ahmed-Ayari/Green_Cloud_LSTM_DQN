# ğŸŒ± Green Cloud Computing: LSTM-DQN VM Consolidation

**Energy Optimization in Data Centers through Dynamic Resource Allocation**

*Master 1 Data Science - Systems Architecture*

[![Python 3.10+](https://img.shields.io/badge/python-3.10+-blue.svg)](https://www.python.org/downloads/)
[![TensorFlow](https://img.shields.io/badge/TensorFlow-2.20-orange.svg)](https://tensorflow.org)
[![PyTorch](https://img.shields.io/badge/PyTorch-2.9-red.svg)](https://pytorch.org)
[![License: MIT](https://img.shields.io/badge/License-MIT-green.svg)](https://opensource.org/licenses/MIT)

---

## ğŸ“‹ Table of Contents

- [Context and Problem Statement](#context-and-problem-statement)
- [Proposed Solution](#proposed-solution)
- [System Architecture](#system-architecture)
- [Installation](#installation)
- [Usage Guide](#usage-guide)
- [Project Structure](#project-structure)
- [Experimental Results](#experimental-results)
- [References](#references)

---

## ğŸ¯ Context and Problem Statement

### The Energy Challenge of Data Centers

Data centers represent **1-2% of global electricity consumption** and this share is growing rapidly. Inefficiency mainly comes from:

- **Underutilized servers**: On average, servers use only 15-20% of their capacity
- **Static allocation**: Resources are often over-provisioned "just in case"
- **Lack of prediction**: Decisions are reactive rather than proactive

### Research Question

> *"How can a dynamic and consolidated resource allocation mechanism be designed to significantly reduce energy consumption while maintaining Quality of Service (QoS)?"*

---

## ğŸ’¡ Proposed Solution

Our **hybrid LSTM-DQN approach** combines two complementary techniques:

### 1. Proactive Prediction (LSTM)

The LSTM (Long Short-Term Memory) network analyzes usage history to predict future load:

```
Historical sequence [t-9, t-8, ..., t] â†’ LSTM â†’ Prediction [t+1]
```

**Advantages**:
- Anticipates load peaks
- Enables preventive consolidation
- Reduces SLA violations

### 2. Autonomous Decision Making (DQN)

The DQN (Deep Q-Network) agent learns the optimal consolidation policy:

```
State (current utilization + predictions) â†’ DQN â†’ Action (migration/consolidation)
```

**Multi-Objective Reward Function**:
```
R = -wâ‚Â·E - wâ‚‚Â·SLA - wâ‚ƒÂ·M

where:
  E   = Energy consumed (normalized)
  SLA = SLA violations
  M   = Number of migrations
  wâ‚, wâ‚‚, wâ‚ƒ = Weights (0.5, 0.3, 0.2 by default)
```

---

## ğŸ—ï¸ System Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                       INPUT DATA                                â”‚
â”‚       PlanetLab Traces (CPU utilization - real data)            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
                           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  PREPROCESSING MODULE                           â”‚
â”‚  â€¢ Trace loading (data_preprocessing.py)                        â”‚
â”‚  â€¢ Normalization [0-100%]                                       â”‚
â”‚  â€¢ VMs â†’ Hosts aggregation                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
                           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  LSTM WORKLOAD PREDICTOR                        â”‚
â”‚  â€¢ Architecture: LSTM(64) â†’ Dropout(0.2) â†’ LSTM(32) â†’ Dense(1)  â”‚
â”‚  â€¢ Input: sequence of 10 timesteps                              â”‚
â”‚  â€¢ Output: prediction t+1 + trend (â†‘â†“â†’)                         â”‚
â”‚  â€¢ File: lstm_predictor.py                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
                           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  DQN CONSOLIDATION AGENT                        â”‚
â”‚  â€¢ Architecture: Linear(state) â†’ ReLU â†’ Linear(128) â†’ Linear(A) â”‚
â”‚  â€¢ Double DQN with Experience Replay                            â”‚
â”‚  â€¢ Actions: do_nothing | migrate(src, dst)                      â”‚
â”‚  â€¢ File: dqn_agent.py                                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
                           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  HYBRID CONTROLLER                              â”‚
â”‚  â€¢ Integrates LSTM + DQN                                        â”‚
â”‚  â€¢ Energy model: P = P_idle + (P_max - P_idle) Ã— U              â”‚
â”‚  â€¢ File: hybrid_controller.py                                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
                           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  METRICS AND EVALUATION                         â”‚
â”‚  â€¢ Total energy (Watts/kWh)                                     â”‚
â”‚  â€¢ SLA violations (% hosts > 80%)                               â”‚
â”‚  â€¢ Number of migrations                                         â”‚
â”‚  â€¢ File: metrics_evaluation.py                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸš€ Installation

### Prerequisites

- **Python** 3.10 or higher
- **pip** (Python package manager)
- **GPU** (optional, speeds up training)

### Step 1: Navigate to Project

```powershell
cd "C:\Users\ahmed\OneDrive\Desktop\M1 Data Science\Architecture SystÃ¨mes\Projet"
```

### Step 2: Create Virtual Environment

```powershell
# Windows PowerShell
python -m venv venv
.\venv\Scripts\activate
```

### Step 3: Install Dependencies

```powershell
pip install -r requirements.txt
```

**Main Dependencies**:
| Package | Version | Usage |
|---------|---------|-------|
| TensorFlow | 2.20.0 | LSTM (Keras) |
| PyTorch | 2.9.1 | DQN |
| NumPy | 1.26+ | Numerical computations |
| Pandas | 2.0+ | Data manipulation |
| Matplotlib | 3.7+ | Visualizations |
| scikit-learn | 1.7+ | Normalization |
| tqdm | 4.65+ | Progress bars |

### Step 4: Download Required Data

#### PlanetLab Workload Traces (Required)

The PlanetLab traces contain real CPU utilization data from distributed systems.

**Download from**: https://github.com/beloglazov/planetlab-workload-traces

```powershell
# Clone the PlanetLab traces repository
git clone https://github.com/beloglazov/planetlab-workload-traces.git temp_planetlab

# Copy traces to the data folder
Copy-Item -Recurse temp_planetlab/* data/planetlab/

# Clean up
Remove-Item -Recurse -Force temp_planetlab
```

**Expected structure after download**:
```
data/planetlab/
â”œâ”€â”€ 20110303/
â”‚   â”œâ”€â”€ trace_file_1
â”‚   â”œâ”€â”€ trace_file_2
â”‚   â””â”€â”€ ...
â”œâ”€â”€ 20110306/
â”œâ”€â”€ 20110309/
â””â”€â”€ ...
```

Each file contains CPU utilization values (one value per line, 0-100%).

#### Google Cluster Data (Optional)

For additional experiments with Google cluster traces:

**Download from**: https://github.com/google/cluster-data

Place CSV files in `data/google_cluster/`.

### Step 5: CloudSim (Optional - Java Simulation)

CloudSim is an optional Java-based cloud simulation toolkit. It's **not required** for running the ML models.

**If needed, download from**: https://github.com/Cloudslab/cloudsim

```powershell
# Clone CloudSim (optional)
git clone https://github.com/Cloudslab/cloudsim.git src/cloudsim
```

---

## ğŸ’» Usage Guide

### Option 1: Quick Test (Recommended to start)

```powershell
cd src
python quick_test.py
```

**Duration**: ~10 minutes  
**Configuration**: 5 hosts, 20 VMs, 5 episodes

**Expected output**:
```
======================================================================
LSTM-DQN VM CONSOLIDATION - QUICK TEST
======================================================================
âœ… Loaded PlanetLab data: (288, 20)
âœ… LSTM training complete!
Episode 1/5: Reward=-6.41, Energy=477.55, Epsilon=1.000
Episode 2/5: Reward=-6.87, Energy=475.15, Epsilon=0.831
...
âœ… Pipeline verification successful!
```

### Option 2: Full Experiment

```powershell
cd src
python main_experiment.py
```

**Duration**: ~2-4 hours (depends on hardware)  
**Configuration**: 10 hosts, 50 VMs, 100 episodes

### Option 3: Programmatic Usage

```python
# 1. Load PlanetLab data
from ml_models.data_preprocessing import WorkloadDataLoader

loader = WorkloadDataLoader('../data')
vm_data = loader.load_planetlab_from_directory(max_traces=50)
print(f"Data loaded: {vm_data.shape}")  # (timesteps, num_vms)

# 2. Train an LSTM predictor
from ml_models.lstm_predictor import LSTMPredictor

predictor = LSTMPredictor(sequence_length=10, lstm_units=64)
predictor.train(vm_data[:, 0], epochs=50, verbose=1)

# Predict next value
sequence = vm_data[-10:, 0]
prediction = predictor.predict(sequence)
trend = predictor.predict_trend(sequence)
print(f"Prediction: {prediction[0]:.2f}%, Trend: {trend}")

# 3. Use the hybrid controller
from ml_models.hybrid_controller import HybridController

controller = HybridController(num_hosts=10, num_vms=50)

# Train LSTMs on historical data
controller.train_lstm_predictors(vm_data, epochs=50)

# Perform a consolidation step
hosts_util = vm_data[100, :10]  # Utilization of 10 hosts
action, reward, metrics = controller.step(hosts_util)

print(f"Action: {action}")
print(f"Energy: {metrics['energy']:.2f} W")
print(f"SLA Violations: {metrics['sla_violations']}")
```

### Option 4: Jupyter Notebooks

```powershell
cd notebooks
jupyter notebook
```

Available notebooks:
1. `01_data_exploration.ipynb` - PlanetLab data exploration
2. `02_lstm_analysis.ipynb` - LSTM predictor analysis
3. `03_dqn_training.ipynb` - DQN agent training
4. `04_results_visualization.ipynb` - Results visualization

---

## ğŸ“ Project Structure

```
Green_Cloud_LSTM_DQN/
â”‚
â”œâ”€â”€ data/                              # Data
â”‚   â””â”€â”€ planetlab/                     # PlanetLab traces
â”‚       â”œâ”€â”€ 20110303/                  # Folder per date
â”‚       â”œâ”€â”€ 20110306/
â”‚       â””â”€â”€ ...
â”‚
â”œâ”€â”€ src/                               # Source code
â”‚   â”œâ”€â”€ ml_models/                     # ML/RL models
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ lstm_predictor.py          # LSTM predictor
â”‚   â”‚   â”œâ”€â”€ dqn_agent.py               # DQN agent (Double DQN)
â”‚   â”‚   â”œâ”€â”€ hybrid_controller.py       # LSTM+DQN controller
â”‚   â”‚   â”œâ”€â”€ data_preprocessing.py      # Data loading
â”‚   â”‚   â””â”€â”€ metrics_evaluation.py      # Metrics calculation
â”‚   â”‚
â”‚   â”œâ”€â”€ main_experiment.py             # Full experiment
â”‚   â”œâ”€â”€ quick_test.py                  # Quick test
â”‚   â””â”€â”€ config.py                      # Global configuration
â”‚
â”œâ”€â”€ notebooks/                         # Jupyter notebooks
â”‚   â”œâ”€â”€ 01_data_exploration.ipynb
â”‚   â”œâ”€â”€ 02_lstm_analysis.ipynb
â”‚   â”œâ”€â”€ 03_dqn_training.ipynb
â”‚   â””â”€â”€ 04_results_visualization.ipynb
â”‚
â”œâ”€â”€ results/                           # Results
â”‚   â”œâ”€â”€ graphs/                        # PNG visualizations
â”‚   â”‚   â””â”€â”€ quick_test_training.png
â”‚   â””â”€â”€ metrics/                       # JSON/CSV metrics
â”‚       â””â”€â”€ quick_test_results.json
â”‚
â”œâ”€â”€ models/                            # Trained models
â”‚   â”œâ”€â”€ lstm_host_*.h5                 # Saved LSTM models
â”‚   â””â”€â”€ dqn_agent.pth                  # Saved DQN agent
â”‚
â”œâ”€â”€ requirements.txt                   # Python dependencies
â”œâ”€â”€ README.md                          # This file
â””â”€â”€ LICENSE                            # MIT License
```

---

## ğŸ“Š Experimental Results

### Test Configuration

| Parameter | Value |
|-----------|-------|
| Data source | PlanetLab (20110303) |
| Number of VMs | 20-50 |
| Number of Hosts | 5-10 |
| LSTM sequence | 10 timesteps |
| DQN episodes | 5-100 |
| Batch size | 32 |

### Quick Test Results

```json
{
  "data_source": "PlanetLab",
  "training_metrics": [
    {"episode": 1, "reward": -6.41, "energy": 477.55, "epsilon": 1.0},
    {"episode": 2, "reward": -6.87, "energy": 475.15, "epsilon": 0.83},
    {"episode": 3, "reward": -4.37, "energy": 484.75, "epsilon": 0.65},
    {"episode": 4, "reward": -3.70, "energy": 483.55, "epsilon": 0.50},
    {"episode": 5, "reward": -2.96, "energy": 484.75, "epsilon": 0.39}
  ],
  "evaluation": {
    "avg_energy": 496.20,
    "avg_sla": 0.0,
    "total_steps": 90
  }
}
```

### Observations

âœ… **Learning**: Reward increases progressively (-6.41 â†’ -2.96)  
âœ… **Exploration â†’ Exploitation**: Epsilon decreases (1.0 â†’ 0.39)  
âœ… **SLA**: No violations detected in evaluation  
âœ… **Energy**: ~480W average for 5 hosts

### Comparison with Baselines (Expected)

| Algorithm | Energy | SLA (%) | Migrations |
|-----------|--------|---------|------------|
| Static Threshold | 100% (baseline) | 15-20% | High |
| Reactive | ~90% | 10-15% | Medium |
| **LSTM-DQN** | **~80%** | **<5%** | **Low** |

*Note: Full results after running the complete experiment.*

---

## ğŸ”¬ Technical Details

### Energy Model

```python
P(u) = P_idle + (P_max - P_idle) Ã— u

where:
  P_idle = 70W   (idle power)
  P_max  = 250W  (maximum power)
  u      = CPU utilization [0, 1]
```

### DQN State Space

```
State = [Uâ‚, Uâ‚‚, ..., Uâ‚™, Pâ‚, Pâ‚‚, ..., Pâ‚™, active_hosts]

where:
  Uáµ¢ = Current utilization of host i
  Páµ¢ = LSTM prediction for host i
  active_hosts = Number of active hosts
```

### DQN Action Space

```
Actions = {
  0: do_nothing,
  1: migrate(host_0 â†’ host_1),
  2: migrate(host_0 â†’ host_2),
  ...
  nÂ²: migrate(host_n-1 â†’ host_n)
}
```

---

## ğŸ“š References

1. **CloudSim**: Calheiros, R. N., et al. "CloudSim: a toolkit for modeling and simulation of cloud computing environments." (2011)

2. **Deep Q-Network**: Mnih, V., et al. "Human-level control through deep reinforcement learning." Nature (2015)

3. **LSTM**: Hochreiter, S., & Schmidhuber, J. "Long short-term memory." Neural Computation (1997)

4. **VM Consolidation**: Beloglazov, A., & Buyya, R. "Optimal online deterministic algorithms for minimizing energy consumption." (2012)

5. **PlanetLab**: Park, K., & Pai, V. S. "CoMon: A mostly-scalable monitoring system for PlanetLab." ACM SIGOPS (2006)

---

## ğŸ“œ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

---

## ğŸ‘¤ Author

**Academic Project - M1 Data Science**  
*Systems Architecture*  
*December 2025*

---

## ğŸ™ Acknowledgments

- PlanetLab data for real workload traces
- TensorFlow and PyTorch open-source community
- CloudSim documentation for simulation concepts

---

**â­ This project demonstrates the application of ML/RL techniques for energy optimization in cloud computing.**
